# LiveInf_SemSeg
New
Create a pretrained weights folder and add your model weights
Also need data directory data/infer/00 to do predictions on local files of any data type
Some of the imports are not properly set


Real Time Semantic Segmentation with ROS Code Report
By: Rahul Andurkar

Set Up for PolarNet
	The first two things that must be installed to use this software is a ROS distro and CUDA. This project was built on ROS noetic and the inference was ran on an Nvidia RTX A4000 (CUDA 11.6). However, your cuda version is dependent on what GPU you have and ROS noetic might be outdated when you read this or it might be better for you to get the program working in ROS melodic if you already have that set up on your device. Currently, ROS noetic can be installed with a single line in your linux terminal: “wget -c https://raw.githubusercontent.com/qboticslabs/ros_install_noetic/master/ros_install_noetic.sh && chmod +x ./ros_install_noetic.sh && ./ros_install_noetic.sh”.

	Your CUDA version determines the versions of pytorch (more on this later) that will allow the program to work so getting this right is crucial and took a lot of time fixing when I initially started this project. Nvidia’s website should tell you what version you need. Here is the cuda installation guide for Linux, https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html. It looks like a lot but it is really simple, only a few lines of commands are needed. You can check what cuda version you have with “nvidia-smi” or “nvcc –version” commands. Your Nvidia driver version and Nvidia cuda version are different but affect each other, here is a link detailing the compatibility of the two, https://docs.nvidia.com/deploy/cuda-compatibility/index.html.

	It is important to note that the project pycharm. For most projects your IDE won’t matter; however, connecting your ROS to your IDE to use rospy or ros in C++ is different for every IDE. There are guides for integration for different IDEs on the ROS wiki, but those are for older distros, here http://wiki.ros.org/IDEs#PyCharm_.28community_edition.29. I followed this simple YouTube tutorial to connect ROS and pycharm, https://www.youtube.com/watch?v=lTew9mbXrAs. Essentially, you extend your python environment  to include the location of all the ROS package folders.

	Most of the python packages required are in the requirements.txt file in the github repository. However, there are a few packages that will not work if you install them that way. Unfortunately, they was a large issue with pytorch and extensions of the package working together; however, I did manage to find a consistent solution regardless of what your OS and GPU is. The following links are to specific torch and then torch- sparse, geometric, scatter, etc libraries. https://mirror.sjtu.edu.cn/pytorch-wheels/?mirror_intel_list will give you pytorch for any OS, CUDA, and python version and  https://pytorch-geometric.com/whl/torch-1.8.1+cu102.html will give you the other libraries for cuda 10.2 and torch 1.8.1. You can change the URL to fit your cuda version and torch version to match what you need. You install these by downloading them and doing “pip install __path_to_whl__” in your IDE terminal. Newer pip versions should be able to automatically download .whl files, in case not, you need to download the wheel package and do “wheel unpack __path_to_whl__”.

	Lastly, you’ll need to clone my github repository for the project, https://github.com/RandyAndy-byte/LiveInf_SemSeg, and install the model weights from the original PolarNet implementation, https://github.com/edwardzhou130/PolarSeg. You’ll need to place the model weights in a new directory call “pretrained-weights”. I plan to add a docker file to do this all automatically, because there are just too many software dependency errors that I had to resolve throughout development.


Running PolarNet
	To run PolarNet inference (assuming all installations are correct) you need to find the PolarNetSK python file under the model_inference directory. Before running any ros commands it is REQUIRED TO RUN “roscore” in a terminal. The program is waiting to receive lidar scan information to run the model on. You can download any sample ouster scan from here, https://data.ouster.io/sample-data-1.13/OS1-64/ , and run it with “rosbag play __path_to_rosbag__”. Now visualization will take some extra steps. In order to perform visualization, you need to run “rosrun rviz rviz”, which will launch the 3D visualization software Rviz. You then need to click the “Add” button in the bottom left of the application and add new data by the name of “labeled/points”. To get a clear visualization, one should switch the shape of the data points to be “point” and change the color channel to “RGB8”. This concludes all the steps to simply run the program. You should get performance ranging from 0.700 – 1.3 s. inference depending on the version of the code and the sample data.

Code Intricacies
	The design philosophy for this software is flexibility and speed, so we can try a variety of different models and maintain real time inference. The inference module is sorted into 3 main parts, initialization, processing, and publishing. The initialization function (“__init__” function) loads the model and its parameters along with the ROS nodes needed to send and receive messages. Also, the dictionaries for converting predicted labels to colors and words are in YAML files are loaded here. The processing section (“ouput” function) is a brief snippet of code that predicts the labels for the point cloud and usually does not change model to model. The data loader is also in this section as each batch of data is a single new scan that was just received. The publishing phase (“run” function) is short, but because it brings all the parts created in the other sections together. I essentially add the colors that I mapped from the output of the model to the original lidar message in proper format and republish it. I attempted to create a super class for a live inference framework where you can change each of these models in the “inference_framework” python file.

	For optimization, you can choose whether or not to load the labels of the predictions, the actual words and names of the objects, or just their colors for the Rviz visualization. You just need to switch which version of the “genColors” function (in data_augmentation.py) you are using by adding the arguments for color and or label. Having both will slow down the inference. It may be beneficial to rewrite the output function to use a queue data loader so that a new data loader object is not created every time, but there isn’t much to rearrange for optimization. Maybe it would be best to cut out ROS for visualization and use Cloud Compares python API.

	For some reason there is a python error or conflict not allowing for the use of roslog to keep track of messages. This is not a big deal, we can just print to a terminal or in the python output section. There were multiple different solutions on github, but it was unclear to me what was the proper solution or how to use them, https://github.com/ros/ros_comm/issues/1384.

Future Work
	As you will see in the model inference folder, more then PolarNet has been tested for live inference with ROS. Cylinder3D is a more modern, and better performing machine learning model for semantic segmentation on lidar scans (10 – 15 % gain). While, this model may or may not run depending on your version of the repository, the output of the model does not make sense and appears randomly generated. There could be a few reasons as to why inference fails; firstly, I know that the model weights fail to load properly because of the trouble I had installing an earlier, unsupported version of the spconv library (version 1.2.0). However, even when I switch to model weights that are updated to be used with spconv version 2 and greater, the program still does not work. If you manage to install spconv 1.2.0 and get the model weights from the original github repository the problem is likely to resolve. The problem also could be that the data is not being loaded properly in the data loader python file. The other model tested, KPRnet, needs arguments in its data augmentation to be altered specifically to the dataset it is being used on. Ideally, KPRnet is tested on data similar to that of which it was trained on (as with all models but it is detrimental in this case). Without changing these parameters in its data loader (all of the hard coded values for 2D projection), the code will run into GPU memory errors due to poor optimization. We should investigate gathering our own data for training and using more state of the art models in the future. When searching online for more applicable datasets to the GRL’s objectives I got brought to an old GRL web page. It appears that the  GRL itself has data that could be useful for training use, but one would need to request whatever that data is specifically.


![rwfwrferf](https://github.com/RandyAndy-byte/LiveInf_SemSeg/assets/80483401/c38c24e3-c11f-42f4-8741-7d27a37e8d94)

I lost the slideshow to show the gifs of the project i had :(



